# ğŸ¤– Agentic AI + MLOps Pipeline with LangChain, Ollama, and MLflow

A production-ready, end-to-end pipeline that combines **Agentic AI workflows** and **modern MLOps practices** using **LangChain**, **Ollama**, **ChromaDB**, **MLflow**, and **Streamlit**.

This project supports intelligent PDF-based assistants using RAG (Retrieval-Augmented Generation), tool-based agents, evaluation, logging, and a beautiful UI.

---

## ğŸ§  Key Features

- ğŸ” PDF Ingestion + Chroma Vector Store  
- ğŸ¤– RAG-based QA with Ollama LLMs (Mistral, LLaMA2, etc.)  
- ğŸ§  Agentic Pipeline with LangGraph + Tools + Memory  
- ğŸ“Š LLM Monitoring with MLflow  
- ğŸ§ª Evaluation with cosine similarity scoring  
- ğŸ–¼ï¸ Streamlit Chatbot UI  
- ğŸ³ Dockerized for deployment  

---

## ğŸ“ Project Structure

â”œâ”€â”€ 01_ingestion/ # Load PDF â†’ Chunk â†’ Embed â†’ Store
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”œâ”€â”€ ingest.py
â”‚ â”‚ â””â”€â”€ utils.py
â”‚ â”œâ”€â”€ data/ # Holds raw PDFs
â”‚ â”œâ”€â”€ embeddings/ # ChromaDB vector index
â”‚ â”œâ”€â”€ query.py # Simple RAG-based QA
â”‚ â”œâ”€â”€ ui.py # Streamlit chatbot
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ 02_agent_pipeline/ # Agentic reasoning pipeline
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ chatbot_agent.py # LangGraph + Tools + Memory
â”‚ â”‚ â””â”€â”€ test_agent.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ 06_monitoring_mlflow/ # LLM monitoring + logging
â”‚ â”œâ”€â”€ log_utils.py
â”‚ â”œâ”€â”€ track_run.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ mlruns/ # Auto-generated MLflow run logs
â”‚
â”œâ”€â”€ 07_evaluation/ # RAG response evaluation
â”‚ â”œâ”€â”€ eval_config.yaml
â”‚ â”œâ”€â”€ evaluate_rag.py
â”‚ â”œâ”€â”€ evaluation_utils.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ Dockerfile # Streamlit + Ollama Docker config
â””â”€â”€ README.md # Youâ€™re reading it!


---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/AnantKrishna1/genai-agentic-mlops.git
cd genai-agentic-mlops

2. (Optional) Create a virtual environment

conda create -n genai-rag python=3.10 -y
conda activate genai-rag

3. Install base requirements

pip install -r 01_ingestion/requirements.txt

4. Run Ollama model (e.g. Mistral)

ollama run mistral

ğŸ”„ Workflow: Step-by-Step
ğŸ“¥ PDF Ingestion & Vector Store
Copy your PDF:

cp sample.pdf 01_ingestion/data/

Run embedding pipeline:

cd 01_ingestion/app
python ingest.py

âœ… Output: Document is split, embedded using SentenceTransformer, and stored in ChromaDB.

ğŸ’¬ Query with RAG
cd ..
python query.py

âœ… Ask questions like:

What is the main purpose of the PDF?

ğŸŒ Streamlit Chatbot UI

streamlit run ui.py

âœ… An interactive chatbot (powered by Ollama + RAG) will launch in your browser.

ğŸ§  Agentic Pipeline with LangGraph

cd ../02_agent_pipeline/
python app/chatbot_agent.py

âœ… Features:

LangGraph-powered memory chain

Tools (Calculator, File Reader, DuckDuckGo Search)

Contextual reasoning over PDF

ğŸ“ˆ LLM Monitoring with MLflow

cd ../06_monitoring_mlflow/
python track_run.py

âœ… Tracks:

Prompt and response

Latency

Similarity metrics

Accuracy

View logs:
mlflow ui

âœ… Evaluation & Benchmarking
cd ../07_evaluation/
python evaluate_rag.py

âœ… Performs:

Cosine similarity scoring

Latency measurement

Pass/Fail based on eval_config.yaml

ğŸ³ Docker Deployment
Build the Docker image:

docker build -t genai-chatbot .

Run the container:
docker run -p 8501:8501 genai-chatbot

âš™ï¸ Optional: Run Ollama container (Mistral)
docker run -d -p 11434:11434 ollama/ollama mistral

ğŸ”¹ Question: What is the main goal of the document?
ğŸ“¤ Model Answer: The document aims to test the ingestion pipeline in an Agentic AI + MLOps project.
âœ… Expected: The document is for testing the ingestion pipeline...
â±ï¸ Response Time: 2.7 seconds
ğŸ“Š Similarity Score: 0.87
ğŸŸ¢ PASSED

ğŸ§° Technologies Used
Category	Tools & Libraries
LLMs	Mistral (via Ollama), LLaMA2, Gemma
Vector DB	ChromaDB + SentenceTransformers
Frameworks	LangChain, LangGraph
Monitoring	MLflow
Frontend	Streamlit
Deployment	Docker

